{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cfa299c",
   "metadata": {},
   "source": [
    "# Building a Preprocessing Pipe for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcb4896b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "Defining Target (dropping neutral reviews):\n",
      "Original_number of rows: 568454 \n",
      "Updated number of rows:  525814\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "Original columns:  Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
      "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text',\n",
      "       'Updated_Score'],\n",
      "      dtype='object')\n",
      "\n",
      "Leaving 'Text','Summary' and 'Score' columns:\n",
      "\n",
      "Selected columns:  Index(['Text', 'Summary', 'Score', 'Updated_Score'], dtype='object')\n",
      "\n",
      "-------------------------------------------------------------------------\n",
      "\n",
      "Total number or reviews: 525814\n",
      "Total number of na (nulls): 25\n",
      "\n",
      "Dropping 25 out of 525814 reviews\n"
     ]
    }
   ],
   "source": [
    "%run ../src/import_libraries.py\n",
    "%matplotlib inline\n",
    "\n",
    "%run ../src/initial_preprocessing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3583a0dc",
   "metadata": {},
   "source": [
    "***\n",
    "## Train/Test Split with Optional Undersampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59256357",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n=20000\n",
    "n=len(df)\n",
    "\n",
    "df_sample=df.sample(n,random_state=12)\n",
    "\n",
    "## Increasing \"weight\" of Summary words:\n",
    "df_sample['Text']= df_sample['Summary']+' '+df_sample['Text']\n",
    "\n",
    "X=df_sample[['Text','Summary']].copy()\n",
    "y=df_sample['Updated_Score']\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.10, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aaea9a",
   "metadata": {},
   "source": [
    "## Pre-processing functions \n",
    "***\n",
    "To be used by vectorizers as a part of prep_pipe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0795def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_review(review, return_list=False):     \n",
    "    review=review.lower()\n",
    "    review_norm=word_tokenize(review)\n",
    "    review_norm  = [SnowballStemmer('english').stem(token) for token in review_norm]\n",
    "    review_norm = [x for x in review_norm if (x.isalpha() & (x not in stop_words) ) ]\n",
    "        \n",
    "    if return_list:\n",
    "        return review_norm\n",
    "    else:\n",
    "        return \" \".join(review_norm)\n",
    "\n",
    "def preprocess_ngram_review(review):\n",
    "    review=review.lower()\n",
    "    words = review.translate(review.maketrans('', '', string.digits+string.punctuation))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531d5d1e",
   "metadata": {},
   "source": [
    "## Custom sklearn classes to build New Features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fedcf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WordCounter class - creates a column counting number of words for each review\n",
    "\n",
    "class WordCounter (BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, data,  y = 0):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data, y = 0):\n",
    "        words_n = data.apply(lambda x: len(x.split()) )\n",
    "        return words_n.values.reshape(-1,1)\n",
    "    \n",
    "## StringCounter class - creates a column counting occurences of any passed string for each review\n",
    "\n",
    "class StringCounter (BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, str_to_count):\n",
    "        self.str_to_count=str_to_count\n",
    "    \n",
    "    def fit(self, data,  y = 0):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data, y = 0):\n",
    "        string_n = data.apply(self.count_string) \n",
    "        return string_n.values.reshape(-1,1)\n",
    "    \n",
    "    def count_string(self, data):\n",
    "        string_n=data.count(self.str_to_count) \n",
    "        total=np.sum([1 for x in data if x.isalpha()]) \n",
    "        if total==0:\n",
    "            total=1\n",
    "        string_p=string_n/total\n",
    "        return string_p \n",
    "    \n",
    "## CapitalCounter class - creates a column counting occurences of Capitalized characters for each review\n",
    "\n",
    "class CapitalCounter (BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, data,  y = 0):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data, y = 0):\n",
    "        capital_count=data.apply(self.count_capital)\n",
    "        return capital_count.values.reshape(-1,1)\n",
    "    \n",
    "    def count_capital(self, data):\n",
    "        capital_n=np.sum([1 for x in data if x.isupper()])\n",
    "        total=np.sum([1 for x in data if x.isalpha()])\n",
    "        if total==0:\n",
    "            total=1\n",
    "        capital_p=capital_n/total\n",
    "        return capital_p \n",
    "\n",
    "## MisspellCounter class - creates a column counting occurences of misspelled words for each review  \n",
    "    \n",
    "class MisspellCounter (BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, data,  y = 0):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data, y = 0):\n",
    "        misspell_count=data.apply(self.count_misspell)\n",
    "        return misspell_count.values.reshape(-1,1)\n",
    "    \n",
    "    def count_misspell(self, data):\n",
    "        misspell_n=np.sum([1 for x in data.split() if x.isalpha() and webster.get(x,0)==0])\n",
    "        total=np.sum([1 for x in data.split() if x.isalpha()])\n",
    "        if total==0:\n",
    "            total=1\n",
    "        misspell_p=misspell_n/total\n",
    "        return misspell_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130656c9",
   "metadata": {},
   "source": [
    "## Loading webster dictionary and stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcf82f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading webster dictionary to identify misspelled words\n",
    "\n",
    "with open('../data/supplementary/webster.json') as data:\n",
    "    webster = json.load(data)\n",
    "    \n",
    "exceptions=[\"against\", \"again\", \"should've\", \"should\", 'because','few']\n",
    "stop_words = stopwords.words('english') # + stop_words\n",
    "for exc in exceptions: stop_words.pop(stop_words.index(exc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6a289d",
   "metadata": {},
   "source": [
    "## Building a Pre-processing Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19b144d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialiazing sklearn classes:\n",
    "\n",
    "text_vectorizer =CountVectorizer(preprocessor=process_review, min_df=0.0004)\n",
    "bi_vectorizer = CountVectorizer( preprocessor=process_ngram_review, ngram_range=(2, 3), min_df=0.0006)\n",
    "bi_summ_vectorizer = CountVectorizer(preprocessor=process_ngram_review, ngram_range=(2, 4), min_df=0.0006)\n",
    "\n",
    "word_counter=WordCounter()\n",
    "quest_counter=StringCounter('?')\n",
    "excl_counter=StringCounter('!')\n",
    "misspell_counter=MisspellCounter()\n",
    "capital_counter=CapitalCounter()\n",
    "\n",
    "scaler=MaxAbsScaler()\n",
    "\n",
    "# FeatureUnion to be used on Text column:\n",
    "\n",
    "text_fu = FeatureUnion([\n",
    "    ('word_counter', word_counter),\n",
    "    ('capital_counter', capital_counter),\n",
    "    ('quest_counter', quest_counter),\n",
    "    ('excl_counter', excl_counter),\n",
    "    ('text_vect', text_vectorizer), \n",
    "    ('bi_text_vect', bi_vectorizer),\n",
    "])\n",
    "\n",
    "# FeatureUnion to be used on Summary column:\n",
    "\n",
    "summ_fu = FeatureUnion([\n",
    "    ('misspell_counter', misspell_counter) ,\n",
    "    ('quest_counter', quest_counter) ,\n",
    "    ('excl_counter', excl_counter),\n",
    "    ('capital_counter',  capital_counter),\n",
    "    ('sum_vect', text_vectorizer), \n",
    "    ('bi_summ_vect', bi_summ_vectorizer)\n",
    "])\n",
    "\n",
    "# ColumnTransformer to combine both FeatureUnions:\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('text_fu', text_fu, 'Text'),\n",
    "    ('summ_fu', summ_fu, 'Summary'),\n",
    "], remainder='passthrough')\n",
    "\n",
    "# Prep_pipe to combine preprocessor and scaler:\n",
    "\n",
    "prep_pipe = Pipeline([('prep', preprocessor),  ('scaler', scaler)\n",
    "                     ])     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a45681d",
   "metadata": {},
   "source": [
    "## Fitting and Transforming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73b29a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:59.205139\n"
     ]
    }
   ],
   "source": [
    "started = datetime.now()\n",
    "\n",
    "prep_train_X=prep_pipe.fit_transform(X_train)\n",
    "prep_test_X=prep_pipe.transform(X_test)\n",
    "\n",
    "print(datetime.now()-started)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c05b7bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 28462)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_test_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dd230f",
   "metadata": {},
   "source": [
    "## Summary:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5b1422",
   "metadata": {},
   "source": [
    "\n",
    "* We have our pre-processing pipeline\n",
    "* New features were carefully tested and selected\n",
    "* Let's try different models (see Modeling.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b4907d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
